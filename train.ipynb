{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from feature_generator import Yelp_Dataset\n",
    "from model_pos_attention import Classifier, BiLSTM, SelfAttentiveEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/scratch/near/anlp/saved_model_pos_attention_find_tune/2/epoch_8_0.78model')\n",
    "model = model.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(Yelp_Dataset('test',cuda),batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6810, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct=0\n",
    "l = []\n",
    "p = []\n",
    "for word_id,pos_id,label in test_loader:\n",
    "    hidden = model.init_hidden(50)\n",
    "    pred, attention,a2 = model(word_id,pos_id,hidden)\n",
    "    pred = torch.max(pred, 1)[1]\n",
    "    p.append(pred.cpu().detach().numpy())\n",
    "    l.append(label.cpu().detach().numpy())\n",
    "    total_correct += torch.sum((pred == label).float())\n",
    "print(total_correct.data / 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = DataLoader(Yelp_Dataset('dev',cuda),batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6755, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct=0\n",
    "for word_id,pos_id,label in dev_loader:\n",
    "    hidden = model.init_hidden(50)\n",
    "    pred, attention,a2 = model(word_id,pos_id,hidden)\n",
    "    pred = torch.max(pred, 1)[1]\n",
    "    total_correct += torch.sum((pred == label).float())\n",
    "print(total_correct.data / 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result for the test set\n",
    "# word2vec only 0.6860\n",
    "# word2vec only fine tune 0.7150\n",
    "# word2vec+pos2vec 0.6600\n",
    "# word2vec+pos2vec fine tune 0.7025\n",
    "# word2vec + pos one_hot 0.6950\n",
    "\n",
    "\n",
    "# word only scratch embedding: 0.7305\n",
    "# pos scratch embedding 0.7205\n",
    "\n",
    "# pos2vec double attention 0.6920\n",
    "# pos one_hot double attention 0.682\n",
    "# pos scratch embedding double attention 0.7305\n",
    "\n",
    "# word2vec one hot fine tune 69.6\n",
    "# word2vec pos2vec attention fine tune: 68.1\n",
    "# word2vec one hot attention fine tune: 56.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result for the dev set\n",
    "# word2vec only 0.6660\n",
    "# word2vec only fine tune 0.7005\n",
    "# word2vec+pos2vec 0.6535\n",
    "# word2vec+pos2vec fine tune 0.6930\n",
    "# word2vec + pos one_hot 0.6720\n",
    "\n",
    "# word only scratch embedding: 0.7300\n",
    "# pos scratch embedding 0.7195\n",
    "\n",
    "# pos2vec double attention 0.6845\n",
    "# pos one_hot double attention 0.6585\n",
    "# pos scratch embedding double attention  0.7350\n",
    "\n",
    "# word2vec one hot fine tune 69.6\n",
    "# word2vec pos2vec attention fine tune: 67.55\n",
    "# word2vec one hot attention fine tune: 55.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = []\n",
    "pp = []\n",
    "for ele in l:\n",
    "    for x in ele:\n",
    "        ll.append(x)\n",
    "for ele in p:\n",
    "    for x in ele:\n",
    "        pp.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "#     fig = plt.figure(figsize=(7,7))\n",
    "#     ax = fig.add_subplot(111)\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "plot_confusion_matrix(ll,pp, classes=np.array([1,2,3,4,5]),\n",
    "                      title='Confusion Matrix of Star Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = torch.load('/scratch/near/anlp/word2vec_matrix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2vec = torch.load('/scratch/near/anlp/pos2vec_matrix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = json.load(open('/scratch/near/anlp/pos_dictionary.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict['NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = []\n",
    "for i in range(pos2vec.shape[0]):\n",
    "    cos = cosine_similarity(pos2vec[3].reshape(1,-1),pos2vec[i].reshape(1,-1))\n",
    "    cs.append((cos,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(reversed(cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {pos_dict[p]:p for p in pos_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    print(i[0],d[i[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
